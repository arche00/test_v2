import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import joblib
import os
import requests
import json
from typing import Optional, Dict, Any
import time
from database import (
    get_db_connection,
    get_pattern_transitions as db_get_pattern_transitions,
    insert_pattern_record,
    cleanup_old_records
)
from psycopg2.extras import DictCursor

# config.json íŒŒì¼ì—ì„œ API í† í° ë¡œë“œ
def load_api_token():
    try:
        # ë¨¼ì € Streamlit secretsì—ì„œ í† í° í™•ì¸
        if 'huggingface_api_token' in st.secrets:
            return st.secrets['huggingface_api_token']
        
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ í† í° í™•ì¸
        token = os.environ.get('HUGGINGFACE_API_TOKEN')
        if token:
            return token
            
        # ë¡œì»¬ config.jsonì—ì„œ í† í° í™•ì¸ (ê°œë°œ í™˜ê²½ìš©)
        if os.path.exists('config.json'):
            with open('config.json', 'r') as f:
                config = json.load(f)
                return config.get('huggingface_api_token')
                
        return None
    except Exception as e:
        print(f"í† í° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

# API í† í° ì´ˆê¸°í™”
if 'hf_api_token' not in st.session_state:
    api_token = load_api_token()
    if api_token:
        st.session_state.hf_api_token = api_token
        print(f"API í† í°ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("API í† í°ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="íŒ¨í„´ ë¶„ì„ ì‹œìŠ¤í…œ",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS ìŠ¤íƒ€ì¼ ì„¤ì •
st.markdown("""
    <style>
    .stText {
        writing-mode: horizontal-tb;
        font-size: 24px;
    }
    .prediction-text {
        font-size: 28px;
        font-weight: bold;
        color: #FF4B4B;
    }
    </style>
""", unsafe_allow_html=True)

def get_pattern_transitions():
    """
    DBì—ì„œ íŒ¨í„´ ì „ì´ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    ìµœê·¼ 150ê°œì™€ ìµœê·¼ 3ì‹œê°„ì˜ ë°ì´í„° ì¤‘ ë” ë§ì€ ê²ƒì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    conn = get_db_connection()
    if conn is None:
        st.warning("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return None

    try:
        with conn.cursor(cursor_factory=DictCursor) as cur:
            # ìµœê·¼ 3ì‹œê°„ ë°ì´í„° ìˆ˜ í™•ì¸
            cur.execute("""
                SELECT COUNT(*) 
                FROM pattern_records 
                WHERE timestamp > CURRENT_TIMESTAMP - INTERVAL '3 hours'
            """)
            recent_count = cur.fetchone()[0]
            
            # ìµœê·¼ 3ì‹œê°„ê³¼ 150ê°œ ì¤‘ ë” í° ê°’ìœ¼ë¡œ ì¡°íšŒ
            if recent_count > 150:
                # ìµœê·¼ 3ì‹œê°„ ë°ì´í„° ì‚¬ìš©
                cur.execute("""
                    SELECT pattern, next_pattern, timestamp
                    FROM pattern_records
                    WHERE timestamp > CURRENT_TIMESTAMP - INTERVAL '3 hours'
                    ORDER BY timestamp DESC
                """)
                st.info(f"ìµœê·¼ 3ì‹œê°„ ë°ì´í„° {recent_count}ê°œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            else:
                # ìµœê·¼ 150ê°œ ë°ì´í„° ì‚¬ìš©
                cur.execute("""
                    SELECT pattern, next_pattern, timestamp
                    FROM pattern_records
                    ORDER BY timestamp DESC
                    LIMIT 150
                """)
                st.info(f"ìµœê·¼ 150ê°œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            
            records = [dict(row) for row in cur.fetchall()]
            
            if not records:
                st.warning("ë°ì´í„°ë² ì´ìŠ¤ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None

            df = pd.DataFrame(records)
            df = df.sort_values(by='timestamp', ascending=True)
            
            return df
            
    except Exception as e:
        st.error(f"ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None
    finally:
        conn.close()

def predict_next_pattern(df: pd.DataFrame, current_pattern: str) -> Optional[Dict[str, Any]]:
    """
    í˜„ì¬ íŒ¨í„´ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ íŒ¨í„´ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
    3ì‹œê°„ ë°ì´í„° ì‚¬ìš© ì‹œ ì‹œê°„ëŒ€ë³„ë¡œ ë‹¤ë¥¸ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.
    """
    if df is None or df.empty or not current_pattern:
        return None
    
    # í˜„ì¬ íŒ¨í„´ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì „ì´ íŒ¨í„´ ì°¾ê¸°
    pattern_data = df[df['pattern'] == current_pattern].copy()
    
    if pattern_data.empty:
        return None
    
    # ë°ì´í„°ê°€ 3ì‹œê°„ ë°ì´í„°ì¸ ê²½ìš°
    pattern_data['timestamp'] = pd.to_datetime(pattern_data['timestamp'])
    time_range = (pattern_data['timestamp'].max() - pattern_data['timestamp'].min()).total_seconds() / 3600
    
    if time_range <= 3:
        # ìµœê·¼ ì‹œê°„ì¼ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬
        now = pattern_data['timestamp'].max()
        pattern_data['time_diff'] = (now - pattern_data['timestamp']).dt.total_seconds() / 60  # ë¶„ ë‹¨ìœ„
        
        # ì‹œê°„ëŒ€ë³„ ê°€ì¤‘ì¹˜ ë¶€ì—¬
        def get_weight(minutes):
            if minutes <= 30:  # ìµœê·¼ 30ë¶„
                return 1.0
            elif minutes <= 60:  # 30ë¶„~1ì‹œê°„
                return 0.8
            elif minutes <= 120:  # 1~2ì‹œê°„
                return 0.6
            else:  # 2~3ì‹œê°„
                return 0.4
        
        pattern_data['weight'] = pattern_data['time_diff'].apply(get_weight)
        
        # ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ ë‹¤ìŒ íŒ¨í„´ ì§‘ê³„
        next_patterns = pd.Series({
            pattern: weights.sum() 
            for pattern, weights in pattern_data.groupby('next_pattern')['weight']
        })
        
        total_weight = pattern_data['weight'].sum()
        confidence = next_patterns.max() / total_weight
        
        # ì‹œê°„ëŒ€ë³„ ë°ì´í„° ìˆ˜ ê³„ì‚° (ë””ë²„ê·¸ìš©)
        time_distribution = {
            '0-30ë¶„': len(pattern_data[pattern_data['time_diff'] <= 30]),
            '30-60ë¶„': len(pattern_data[(pattern_data['time_diff'] > 30) & (pattern_data['time_diff'] <= 60)]),
            '1-2ì‹œê°„': len(pattern_data[(pattern_data['time_diff'] > 60) & (pattern_data['time_diff'] <= 120)]),
            '2-3ì‹œê°„': len(pattern_data[pattern_data['time_diff'] > 120])
        }
    else:
        # 3ì‹œê°„ ì´ìƒì˜ ë°ì´í„°ì¸ ê²½ìš° ê¸°ì¡´ ë°©ì‹ëŒ€ë¡œ ì²˜ë¦¬
        next_patterns = pattern_data['next_pattern'].value_counts()
        confidence = next_patterns.iloc[0] / len(pattern_data)
        time_distribution = None
    
    best_next = next_patterns.index[0]
    
    # ì‹ ë¢°ë„ê°€ 50% ë¯¸ë§Œì´ë©´ ë°˜ëŒ€ íŒ¨í„´ì´ ë” ë†’ì€ í™•ë¥ 
    if confidence < 0.5:
        best_next = 'b' if best_next == 'a' else 'a'
        confidence = 1 - confidence
    
    # ë””ë²„ê·¸ ì •ë³´ ì¶”ê°€
    debug_info = {
        'pattern': current_pattern,
        'total_matches': len(pattern_data),
        'next_patterns': next_patterns.to_dict(),
        'confidence_adjusted': confidence >= 0.5,
        'using_weighted_calc': time_range <= 3,
        'time_distribution': time_distribution
    }
    
    return {
        'next_pattern': best_next,
        'confidence': confidence,
        'method': 'ì‹œê°„ëŒ€ë³„ ê°€ì¤‘ì¹˜' if time_range <= 3 else 'ë¹ˆë„ ê¸°ë°˜',
        'debug_info': debug_info
    }

def predict_next_pattern2(df, current_pattern1, current_pattern2):
    """
    í˜„ì¬ íŒ¨í„´1ê³¼ íŒ¨í„´2ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ íŒ¨í„´ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
    """
    if df is None or df.empty or not current_pattern1 or not current_pattern2:
        return None
    
    # í˜„ì¬ íŒ¨í„´1ê³¼ íŒ¨í„´2ë¡œ ì‹œì‘í•˜ëŠ” ì „ì´ íŒ¨í„´ ì°¾ê¸°
    transitions = df[(df['prev_pattern1'] == current_pattern1) & 
                    (df['prev_pattern2'] == current_pattern2)]
    if transitions.empty:
        return None
    
    # ê°€ì¥ ë¹ˆë²ˆí•œ ë‹¤ìŒ íŒ¨í„´ ì°¾ê¸°
    next_patterns = transitions['pattern1'].value_counts()
    if next_patterns.empty:
        return None
    
    # ì˜ˆì¸¡ ê²°ê³¼ ë°˜í™˜
    return {
        'next_pattern': next_patterns.index[0],
        'confidence': next_patterns.iloc[0] / len(transitions),
        'total_occurrences': len(transitions)
    }

def prepare_training_data():
    """
    ML ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤.
    """
    try:
        conn = get_db_connection()
        
        # ì „ì²´ ë°ì´í„° ì¡°íšŒ
        query = '''
            SELECT pattern, next_pattern, timestamp
            FROM pattern_records
            ORDER BY timestamp
        '''
        df = pd.read_sql_query(query, conn)
        conn.close()
        
        if df.empty:
            st.warning("í•™ìŠµí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None, None, None
        
        # íŒ¨í„´ íŠ¹ì„± ê³„ì‚°
        df['banker_count'] = df['pattern'].apply(lambda x: x.count('a'))
        df['player_count'] = df['pattern'].apply(lambda x: x.count('b'))
        df['transitions'] = df['pattern'].apply(lambda x: sum(1 for i in range(len(x)-1) if x[i] != x[i+1]))
        
        # íŠ¹ì„±(X)ê³¼ ë ˆì´ë¸”(y) ì¤€ë¹„
        features = ['banker_count', 'player_count', 'transitions']
        
        X = df[features].values
        y = df['next_pattern'].values  # ë‹¤ìŒ íŒ¨í„´ ì˜ˆì¸¡
        
        return X, y, features
    except Exception as e:
        st.error(f"í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None, None, None

def train_ml_model():
    """
    RandomForest ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤.
    """
    X, y, features = prepare_training_data()
    if X is None or y is None:
        return None
    
    # ë ˆì´ë¸” ì¸ì½”ë”©
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    
    # ëª¨ë¸ í•™ìŠµ
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X, y_encoded)
    
    # ëª¨ë¸ ì €ì¥
    model_path = 'pattern_prediction_model.joblib'
    joblib.dump((model, le, features), model_path)
    
    return model, le, features

def get_ml_predictions(patterns):
    """ML ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ íŒ¨í„´ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    ml_data = []
    for pattern in patterns:
        prediction = predict_with_ml(pattern)
        if prediction:
            ml_data.append({
                "íŒ¨í„´": pattern,
                "ì˜ˆì¸¡ê°’": prediction['next_pattern'],
                "ì‹ ë¢°ë„": prediction['confidence']
            })
    return ml_data

def display_ml_prediction_table(df):
    """
    ML ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
    """
    st.markdown("## ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼")
    
    # íŒ¨í„´1 (4ê°œ: aa, ab, ba, bb)
    pattern1_list = ['aa', 'ab', 'ba', 'bb']
    
    # íŒ¨í„´2 (8ê°œ: aaa, aab, aba, abb, baa, bab, bba, bbb)
    pattern2_list = ['aaa', 'aab', 'aba', 'abb', 'baa', 'bab', 'bba', 'bbb']
    
    # íŒ¨í„´1ê³¼ íŒ¨í„´2 ì˜ˆì¸¡ì„ ìˆ˜í‰ìœ¼ë¡œ ë°°ì¹˜
    col1, col2 = st.columns([3, 3])
    
    # íŒ¨í„´1 ì˜ˆì¸¡
    with col1:
        st.markdown("### íŒ¨í„´1 ML ì˜ˆì¸¡ (aa, ab, ba, bb)")
        ml_data1 = get_ml_predictions(pattern1_list)
        if ml_data1:
            df1 = pd.DataFrame(ml_data1)
            df1['ì‹ ë¢°ë„'] = df1['ì‹ ë¢°ë„'].apply(lambda x: f"{x:.1%}")
            st.table(df1)
        else:
            st.info("ML ì˜ˆì¸¡ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # íŒ¨í„´2 ì˜ˆì¸¡
    with col2:
        st.markdown("### íŒ¨í„´2 ML ì˜ˆì¸¡ (aaa ~ bbb)")
        ml_data2 = get_ml_predictions(pattern2_list)
        if ml_data2:
            df2 = pd.DataFrame(ml_data2)
            df2['ì‹ ë¢°ë„'] = df2['ì‹ ë¢°ë„'].apply(lambda x: f"{x:.1%}")
            st.table(df2)
        else:
            st.info("ML ì˜ˆì¸¡ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

def predict_with_ml(current_pattern1, current_pattern2=None):
    """
    ML ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ íŒ¨í„´ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
    """
    try:
        # ëª¨ë¸ ë¡œë“œ
        model_path = 'pattern_prediction_model.joblib'
        if not os.path.exists(model_path):
            model, le, features = train_ml_model()
        else:
            model, le, features = joblib.load(model_path)
        
        # í˜„ì¬ íŒ¨í„´ì˜ íŠ¹ì„± ê³„ì‚°
        pattern_features = {
            'banker_count': current_pattern1.count('a'),
            'player_count': current_pattern1.count('b'),
            'transitions': sum(1 for i in range(len(current_pattern1)-1) if current_pattern1[i] != current_pattern1[i+1])
        }
        
        # ì˜ˆì¸¡
        X_pred = np.array([pattern_features[f] for f in features]).reshape(1, -1)
        y_pred = model.predict_proba(X_pred)
        
        # ê²°ê³¼ ë³€í™˜
        predicted_class = le.inverse_transform([np.argmax(y_pred)])[0]
        confidence = np.max(y_pred)
        
        return {
            'next_pattern': predicted_class,
            'confidence': confidence,
            'method': 'ML Model (RandomForest)'
        }
        
    except Exception as e:
        st.error(f"ML ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

def get_huggingface_prediction(pattern: str) -> Optional[Dict[str, Any]]:
    """
    Hugging Face APIë¥¼ ì‚¬ìš©í•˜ì—¬ íŒ¨í„´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    API_TOKEN = st.session_state.get("hf_api_token")
    if not API_TOKEN:
        return None

    # íŒ¨í„´ ë¶„ì„ìš© ëª¨ë¸ ì„ íƒ (text-classification ëª¨ë¸ ì‚¬ìš©)
    API_URL = "https://api-inference.huggingface.co/models/facebook/bart-large-mnli"
    headers = {"Authorization": f"Bearer {API_TOKEN}"}
    
    try:
        # ì…ë ¥ ë°ì´í„° êµ¬ì„±
        payload = {
            "inputs": pattern,
            "parameters": {
                "candidate_labels": ["next_a", "next_b"]
            }
        }
        
        response = requests.post(API_URL, headers=headers, json=payload)
        
        if response.status_code == 200:
            result = response.json()
            
            # API ì‘ë‹µ ê²€ì¦
            if isinstance(result, dict) and 'scores' in result and 'labels' in result:
                # ìµœê³  í™•ë¥ ì˜ ì˜ˆì¸¡ê°’ ì„ íƒ
                max_score_idx = result['scores'].index(max(result['scores']))
                predicted_value = 'a' if result['labels'][max_score_idx] == 'next_a' else 'b'
                confidence = result['scores'][max_score_idx]
                
                # ì‹ ë¢°ë„ê°€ 50% ë¯¸ë§Œì´ë©´ ë°˜ëŒ€ íŒ¨í„´ ì„ íƒ
                if confidence < 0.5:
                    predicted_value = 'b' if predicted_value == 'a' else 'a'
                    confidence = 1 - confidence
                
                return {
                    'next_pattern': predicted_value,
                    'confidence': confidence,
                    'method': 'Hugging Face API',
                    'model_name': 'BART Large MNLI',
                    'raw_predictions': {
                        'next_a': result['scores'][result['labels'].index('next_a')],
                        'next_b': result['scores'][result['labels'].index('next_b')]
                    }
                }
            else:
                st.error(f"ì˜ˆìƒì¹˜ ëª»í•œ API ì‘ë‹µ í˜•ì‹: {result}")
                return None
        else:
            st.error(f"API ì˜¤ë¥˜: {response.status_code} - {response.text}")
            return None
            
    except Exception as e:
        st.error(f"API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

def find_similar_patterns(df: pd.DataFrame, pattern: str) -> Optional[Dict[str, Any]]:
    """
    ìœ ì‚¬í•œ íŒ¨í„´ì„ ì°¾ì•„ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    if df is None or df.empty:
        return None
        
    # íŒ¨í„´ ê¸¸ì´ì™€ êµ¬ì„±ì´ ë¹„ìŠ·í•œ íŒ¨í„´ë“¤ì„ ì°¾ìŒ
    similar_patterns = df[
        (df['pattern1'].str.len() == len(pattern)) &
        (df['pattern1'].str.count('a') == pattern.count('a'))
    ]
    
    if similar_patterns.empty:
        return None
        
    # ê°€ì¥ ë¹ˆë²ˆí•œ ë‹¤ìŒ íŒ¨í„´ ì°¾ê¸°
    next_patterns = similar_patterns['result1'].value_counts()
    
    return {
        'next_pattern': next_patterns.index[0],
        'confidence': next_patterns.iloc[0] / len(similar_patterns),
        'method': 'ìœ ì‚¬ íŒ¨í„´ ê¸°ë°˜'
    }

def clear_database():
    """
    ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    """
    return cleanup_old_records()

def update_database():
    """
    ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ìµœì‹  ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    """
    try:
        if cleanup_old_records():
            st.success("ë°ì´í„°ë² ì´ìŠ¤ê°€ ì„±ê³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
            st.experimental_rerun()  # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨
        else:
            st.error("ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        st.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

def analyze_pattern_combination(df: pd.DataFrame, pattern1: str, pattern2: str) -> Optional[Dict]:
    """
    ë‘ íŒ¨í„´ì˜ ì¡°í•©ì— ëŒ€í•œ í†µê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    """
    if df is None or df.empty:
        return None
        
    # íŒ¨í„´ ì¡°í•© ì°¾ê¸°
    combined = df[
        (df['pattern'] == pattern1) & (df['next_pattern'] == pattern2)
    ]
    
    if combined.empty:
        return None
    
    # í†µê³„ ê³„ì‚°
    total_occurrences = len(combined)
    
    # ì „ì²´ íŒ¨í„´ ì¤‘ í˜„ì¬ ì¡°í•©ì˜ ë¹„ìœ¨ ê³„ì‚°
    total_patterns = len(df)
    sequential_prob = total_occurrences / total_patterns if total_patterns > 0 else 0
    
    # í‰ê·  ì „í™˜ íšŸìˆ˜ (í˜„ì¬ëŠ” í•­ìƒ 1, ë‚˜ì¤‘ì— í•„ìš”í•˜ë©´ ìˆ˜ì •)
    avg_transitions = 1
    
    return {
        'total_occurrences': total_occurrences,
        'sequential_probability': sequential_prob,
        'avg_transitions': avg_transitions
    }

def create_comparison_data(local_data, api_data):
    """
    ë¡œì»¬ê³¼ API ì˜ˆì¸¡ì„ ë¹„êµí•˜ì—¬ ì¼ì¹˜í•˜ëŠ” í•­ëª©ê³¼ ì°¨ì´ê°€ ìˆëŠ” í•­ëª©ì„ ë¶„ë¦¬í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    matching_predictions = []
    differing_predictions = []
    
    if not api_data:
        return matching_predictions, differing_predictions
        
    local_dict = {item["íŒ¨í„´"]: item for item in local_data}
    api_dict = {item["íŒ¨í„´"]: item for item in api_data}
    
    for pattern in local_dict.keys():
        if pattern in api_dict:
            local_pred = local_dict[pattern]
            api_pred = api_dict[pattern]
            comparison_item = {
                "íŒ¨í„´": pattern,
                "ë¡œì»¬ ì˜ˆì¸¡": local_pred["ì˜ˆì¸¡ê°’"],
                "ë¡œì»¬ ì‹ ë¢°ë„": f"{local_pred['ì‹ ë¢°ë„']:.1%}",
                "API ì˜ˆì¸¡": api_pred["ì˜ˆì¸¡ê°’"],
                "API ì‹ ë¢°ë„": f"{api_pred['ì‹ ë¢°ë„']:.1%}"
            }
            if local_pred["ì˜ˆì¸¡ê°’"] == api_pred["ì˜ˆì¸¡ê°’"]:
                matching_predictions.append(comparison_item)
            else:
                differing_predictions.append(comparison_item)
                
    return matching_predictions, differing_predictions

def get_local_predictions(patterns, df):
    """ë¡œì»¬ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    local_data = []
    for pattern in patterns:
        prediction = predict_next_pattern(df, pattern)
        if prediction:
            local_data.append({
                "íŒ¨í„´": pattern,
                "ì˜ˆì¸¡ê°’": prediction['next_pattern'],
                "ì‹ ë¢°ë„": prediction['confidence']
            })
    return local_data

def get_api_predictions(patterns):
    """API ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³  ì˜¤ë¥˜ ë°œìƒ ì‹œ ì ì ˆí•œ ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    api_data = []
    api_error = None
    if "hf_api_token" in st.session_state:
        try:
            for pattern in patterns:
                prediction = get_huggingface_prediction(pattern)
                if prediction:
                    api_data.append({
                        "íŒ¨í„´": pattern,
                        "ì˜ˆì¸¡ê°’": prediction['next_pattern'],
                        "ì‹ ë¢°ë„": prediction['confidence']
                    })
        except Exception as e:
            api_error = f"API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    return api_data, api_error

def display_pattern_prediction_table(df):
    """
    íŒ¨í„´1(4ê°œ)ê³¼ íŒ¨í„´2(8ê°œ)ì˜ ì˜ˆì¸¡ê°’ì„ í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ ìˆ˜í‰ ë°°ì¹˜í•˜ì—¬ í‘œì‹œí•©ë‹ˆë‹¤.
    """
    st.markdown("## íŒ¨í„´ ì˜ˆì¸¡ ë¹„êµ í…Œì´ë¸”")
    
    # íŒ¨í„´1 (4ê°œ: aa, ab, ba, bb)
    pattern1_list = ['aa', 'ab', 'ba', 'bb']
    
    # íŒ¨í„´2 (8ê°œ: aaa, aab, aba, abb, baa, bab, bba, bbb)
    pattern2_list = ['aaa', 'aab', 'aba', 'abb', 'baa', 'bab', 'bba', 'bbb']
    
    # íŒ¨í„´1ê³¼ íŒ¨í„´2 ì˜ˆì¸¡ì„ ìˆ˜í‰ìœ¼ë¡œ ë°°ì¹˜
    col1, col2 = st.columns([3, 3])
    
    # íŒ¨í„´1 ì˜ˆì¸¡
    with col1:
        st.markdown("### íŒ¨í„´1 ì˜ˆì¸¡ (aa, ab, ba, bb)")
        
        # ë¡œì»¬ê³¼ API ì˜ˆì¸¡ì„ ë‚˜ë€íˆ í‘œì‹œ
        subcol1, subcol2 = st.columns(2)
        
        # ë¡œì»¬ ì˜ˆì¸¡ (í•­ìƒ í‘œì‹œ)
        with subcol1:
            st.markdown("#### ë¡œì»¬ ê¸°ë°˜ ì˜ˆì¸¡")
            local_data1 = get_local_predictions(pattern1_list, df)
            if local_data1:
                st.table(pd.DataFrame(local_data1))
            else:
                st.info("ì˜ˆì¸¡ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # API ì˜ˆì¸¡
        with subcol2:
            st.markdown("#### API ê¸°ë°˜ ì˜ˆì¸¡")
            api_data1, api_error1 = get_api_predictions(pattern1_list)
            if api_error1:
                st.error(api_error1)
            elif api_data1:
                st.table(pd.DataFrame(api_data1))
            else:
                st.info("API í† í°ì„ ì„¤ì •í•˜ì„¸ìš”.")
        
        # ì˜ˆì¸¡ ë¹„êµ ë¶„ì„ (ì¼ì¹˜/ì°¨ì´ ë¶„ë¦¬)
        matching1, differing1 = create_comparison_data(local_data1, api_data1)
        
        if matching1:
            st.markdown("#### ì˜ˆì¸¡ ì¼ì¹˜ (ë¡œì»¬ == API)")
            st.table(pd.DataFrame(matching1))
        
        if differing1:
            st.markdown("#### ì˜ˆì¸¡ ì°¨ì´ (ë¡œì»¬ != API)")
            st.table(pd.DataFrame(differing1))
    
    # íŒ¨í„´2 ì˜ˆì¸¡
    with col2:
        st.markdown("### íŒ¨í„´2 ì˜ˆì¸¡ (aaa ~ bbb)")
        
        # ë¡œì»¬ê³¼ API ì˜ˆì¸¡ì„ ë‚˜ë€íˆ í‘œì‹œ
        subcol1, subcol2 = st.columns(2)
        
        # ë¡œì»¬ ì˜ˆì¸¡ (í•­ìƒ í‘œì‹œ)
        with subcol1:
            st.markdown("#### ë¡œì»¬ ê¸°ë°˜ ì˜ˆì¸¡")
            local_data2 = get_local_predictions(pattern2_list, df)
            if local_data2:
                st.table(pd.DataFrame(local_data2))
            else:
                st.info("ì˜ˆì¸¡ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # API ì˜ˆì¸¡
        with subcol2:
            st.markdown("#### API ê¸°ë°˜ ì˜ˆì¸¡")
            api_data2, api_error2 = get_api_predictions(pattern2_list)
            if api_error2:
                st.error(api_error2)
            elif api_data2:
                st.table(pd.DataFrame(api_data2))
            else:
                st.info("API í† í°ì„ ì„¤ì •í•˜ì„¸ìš”.")
        
        # ì˜ˆì¸¡ ë¹„êµ ë¶„ì„ (ì¼ì¹˜/ì°¨ì´ ë¶„ë¦¬)
        matching2, differing2 = create_comparison_data(local_data2, api_data2)
        
        if matching2:
            st.markdown("#### ì˜ˆì¸¡ ì¼ì¹˜ (ë¡œì»¬ == API)")
            st.table(pd.DataFrame(matching2))
        
        if differing2:
            st.markdown("#### ì˜ˆì¸¡ ì°¨ì´ (ë¡œì»¬ != API)")
            st.table(pd.DataFrame(differing2))

def main():
    st.title("íŒ¨í„´ ë¶„ì„ ì‹œìŠ¤í…œ")
    
    # ë°ì´í„° ë¡œë“œ
    df = get_pattern_transitions()
    if df is None:
        return
    
    # DB ê´€ë¦¬ ë²„íŠ¼ë“¤
    col1, col2 = st.columns(2)
    with col1:
        if st.button("DB ì—…ë°ì´íŠ¸"):
            if update_database():
                st.success("ë°ì´í„°ë² ì´ìŠ¤ê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
                st.experimental_rerun()
    with col2:
        if st.button("ML ëª¨ë¸ ì¬í•™ìŠµ"):
            with st.spinner("ëª¨ë¸ í•™ìŠµ ì¤‘..."):
                model, le, features = train_ml_model()
                if model is not None:
                    st.success("ëª¨ë¸ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    # ì˜ˆì¸¡ê°’ í…Œì´ë¸” í‘œì‹œ
    display_pattern_prediction_table(df)
    
    st.markdown("---")
    
    # ML ì˜ˆì¸¡ ê²°ê³¼ í…Œì´ë¸” í‘œì‹œ
    display_ml_prediction_table(df)

if __name__ == "__main__":
    main() 